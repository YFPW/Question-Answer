{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESIM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMDXWfVUBcVD",
        "colab_type": "code",
        "outputId": "bf6369e9-e336-4274-c43a-3e20b7268afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "from time import time\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import itertools\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Input, Dot, Softmax, Multiply, Concatenate, Subtract, Dense, Lambda, Embedding, Dropout\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD, Adam\n",
        "import keras\n",
        "\n",
        "#download the pre-trained model from gensim\n",
        "word_embed_model = api.load('glove-wiki-gigaword-300')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQX3wf9bcoYe",
        "colab_type": "text"
      },
      "source": [
        "##The code is run on the Google colab, so the notebook is linked with my Google drive to read training, testing, development set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOlq4GHBH4ko",
        "colab_type": "code",
        "outputId": "75095817-5198-4dc7-89e8-34689fe9a2da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEQzImTcdBQ6",
        "colab_type": "text"
      },
      "source": [
        "read all files from Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehGw-cfABgIw",
        "colab_type": "code",
        "outputId": "a643f2f1-a57a-4c01-9e9d-83d111ee405c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/train4.csv')\n",
        "dev_df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/devset.csv')\n",
        "test_df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/testset3.csv')\n",
        "'''\n",
        "train_df = pd.read_csv('/content/gdrive/My Drive/esim_output/num_train.csv')\n",
        "dev_df = pd.read_csv('/content/gdrive/My Drive/esim_output/num_dev.csv')\n",
        "test_df = pd.read_csv('/content/gdrive/My Drive/esim_output/num_test.csv')\n",
        "embeddings = np.load('/content/gdrive/My Drive/esim_output/embeddings.npy')'''\n",
        "print(train_df.shape)\n",
        "print(dev_df.shape)\n",
        "print(test_df.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150000, 3)\n",
            "(120581, 6)\n",
            "(584843, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6MkoefWCOF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the fucntion is used to changed the string labels into numbers\n",
        "def change_label(df):\n",
        "  labels = df['label']\n",
        "  new_labels = []\n",
        "  for row in labels:\n",
        "    if row == 'SUPPORTS':\n",
        "      new_labels.append(0)\n",
        "    elif row == 'REFUTES':\n",
        "      new_labels.append(1)\n",
        "    else:\n",
        "      new_labels.append(2)\n",
        "  df['label'] = pd.Series(new_labels)\n",
        "\n",
        "change_label(train_df)\n",
        "change_label(dev_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URDgDtMfCaqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the function is to do the same process as the data retriever for the claims\n",
        "def process_text(raw_text):\n",
        "    text = str(raw_text)\n",
        "    text = re.sub('_', ' ', text)\n",
        "    text = re.sub('-LRB-', ' ', text)\n",
        "    text = re.sub('-RRB-', ' ', text)\n",
        "    text = re.sub('-LSB-', ' ', text)\n",
        "    text = re.sub('-RSB-', ' ', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.split()\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25hb1EnmdYxM",
        "colab_type": "text"
      },
      "source": [
        "The vocab stores all the vocabulary in all sets and the index stores the position of the word in the embedding matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHXjZu0OCgOf",
        "colab_type": "code",
        "outputId": "cd8228f6-cdd4-49ee-cdb1-b99e044317ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "vocab = {}\n",
        "vocab_index = ['<unk>']\n",
        "\n",
        "#substitute the sentence in the train data into the index representation\n",
        "for index, row in train_df.iterrows():\n",
        "  if index%100000 == 0:\n",
        "    print(index)\n",
        "  for sentence in ['claim', 'evidence']:\n",
        "    number_rep = []\n",
        "    for word in process_text(row[sentence]):\n",
        "      if word in stopwords.words('english') and word not in word_embed_model.vocab:\n",
        "        continue\n",
        "      if word not in vocab:\n",
        "        vocab[word] = len(vocab_index)\n",
        "        number_rep.append(len(vocab_index))\n",
        "        vocab_index.append(word)\n",
        "      else:\n",
        "        number_rep.append(vocab[word])\n",
        "    train_df.at[index, sentence] = number_rep\n",
        "    \n",
        "#substitute the sentence in the test data into the index representation\n",
        "for index, row in test_df.iterrows():\n",
        "  if index%100000 == 0:\n",
        "    print(index)\n",
        "  for sentence in ['claim', 'evidence']:\n",
        "    number_rep = []\n",
        "    for word in process_text(row[sentence]):\n",
        "      if word in stopwords.words('english') and word not in word_embed_model.vocab:\n",
        "        continue\n",
        "      if word not in vocab:\n",
        "        vocab[word] = len(vocab_index)\n",
        "        number_rep.append(len(vocab_index))\n",
        "        vocab_index.append(word)\n",
        "      else:\n",
        "        number_rep.append(vocab[word])\n",
        "    test_df.at[index, sentence] = number_rep\n",
        "    \n",
        "#substitute the sentence in the dev data into the index representation   \n",
        "for index, row in dev_df.iterrows():\n",
        "  if index%100000 == 0:\n",
        "    print(index)\n",
        "  for sentence in ['claim', 'evidence']:\n",
        "    number_rep = []\n",
        "    for word in process_text(row[sentence]):\n",
        "      if word in stopwords.words('english') and word not in word_embed_model.vocab:\n",
        "        continue\n",
        "      if word not in vocab:\n",
        "        vocab[word] = len(vocab_index)\n",
        "        number_rep.append(len(vocab_index))\n",
        "        vocab_index.append(word)\n",
        "      else:\n",
        "        number_rep.append(vocab[word])\n",
        "    dev_df.at[index, sentence] = number_rep   \n",
        "    \n",
        "#embedding all vocabulary in a matrix\n",
        "dimension = 300        \n",
        "embeddings = 1 * np.random.randn(len(vocab) + 1,dimension)\n",
        "embeddings[0] = 0 \n",
        "for word, index in vocab.items():\n",
        "  if word in word_embed_model.vocab:\n",
        "    embeddings[index] = word_embed_model[word]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100000\n",
            "0\n",
            "100000\n",
            "200000\n",
            "300000\n",
            "400000\n",
            "500000\n",
            "0\n",
            "100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgO4wKGkCD5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save all middle files to save time when the colab reinitialized\n",
        "np.save('/content/gdrive/My Drive/esim_output/embeddings.npy',embeddings)\n",
        "train_df.to_csv('/content/gdrive/My Drive/esim_output/num_train.csv')\n",
        "test_df.to_csv('/content/gdrive/My Drive/esim_output/num_test3.csv')\n",
        "dev_df.to_csv('/content/gdrive/My Drive/esim_output/num_dev.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9Zt_rifChKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find the maximum length in the test and training set\n",
        "def max_sequence(df1,df2):\n",
        "    max_seq = max(df1.claim.map(lambda x: len(x)).max(), df1.evidence.map(lambda x: len(x)).max(),df2.claim.map(lambda x: len(x)).max(), df2.evidence.map(lambda x: len(x)).max())\n",
        "    return max_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY8cCftTEQEw",
        "colab_type": "code",
        "outputId": "a4cba99f-6fda-4df4-fb14-cce0da3b6378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "# define the ESIM model in keras\n",
        "n_epoch = 4\n",
        "max_seq = max_sequence(train_df, test_df)\n",
        "dimension = 300\n",
        "c_input = Input(shape=(max_seq,), dtype='float32')\n",
        "e_input = Input(shape=(max_seq,), dtype='float32')\n",
        "embedding_layer = Embedding(len(embeddings), dimension, weights=[embeddings], input_length=max_seq, trainable=False)\n",
        "c_embed = embedding_layer(c_input)\n",
        "e_embed = embedding_layer(e_input)\n",
        "\n",
        "x1 = Bidirectional(LSTM(100, return_sequences=True))(c_embed)\n",
        "x2 = Bidirectional(LSTM(100, return_sequences=True))(e_embed)\n",
        "e = Dot(axes=2)([x1, x2])\n",
        "\n",
        "e1 = Softmax(axis=2)(e)\n",
        "e2 = Softmax(axis=1)(e)\n",
        "e1 = Lambda(K.expand_dims, arguments={'axis' : 3})(e1)\n",
        "e2 = Lambda(K.expand_dims, arguments={'axis' : 3})(e2)\n",
        "\n",
        "_x1 = Lambda(K.expand_dims, arguments={'axis' : 1})(x2)\n",
        "_x1 = Multiply()([e1, _x1])\n",
        "_x1 = Lambda(K.sum, arguments={'axis' : 2})(_x1)\n",
        "_x2 = Lambda(K.expand_dims, arguments={'axis' : 2})(x1)\n",
        "_x2 = Multiply()([e2, _x2])\n",
        "_x2 = Lambda(K.sum, arguments={'axis' : 1})(_x2)\n",
        "\n",
        "m1 = Concatenate()([x1, _x1, Subtract()([x1, _x1]), Multiply()([x1, _x1])])\n",
        "m2 = Concatenate()([x2, _x2, Subtract()([x2, _x2]), Multiply()([x2, _x2])])\n",
        "\n",
        "y1 = Bidirectional(LSTM(100, return_sequences=True))(m1)\n",
        "y2 = Bidirectional(LSTM(100, return_sequences=True))(m2)\n",
        "\n",
        "mx1 = Lambda(K.max, arguments={'axis' : 1})(y1)\n",
        "av1 = Lambda(K.mean, arguments={'axis' : 1})(y1)\n",
        "mx2 = Lambda(K.max, arguments={'axis' : 1})(y2)\n",
        "av2 = Lambda(K.mean, arguments={'axis' : 1})(y2)\n",
        "\n",
        "y = Concatenate()([av1, mx1, av2, mx2])\n",
        "y = Dense(128, activation='tanh')(y)\n",
        "y = Dropout(0.5)(y)\n",
        "y = Dense(128, activation='tanh')(y)\n",
        "y = Dropout(0.5)(y)\n",
        "y = Dense(3, activation='softmax')(y)\n",
        "model = Model(inputs=[c_input, e_input], outputs=y)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIAmLimNFkDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure all training input for the model has the max_seq length\n",
        "cols = ['claim','evidence']\n",
        "x = train_df[cols]\n",
        "y = train_df['label']\n",
        "\n",
        "\n",
        "v_size = 15000\n",
        "t_size = len(train_df) - v_size\n",
        "    \n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=v_size)\n",
        "x_train = {'claim': x_train.claim, 'evidence': x_train.evidence}\n",
        "x_val = {'claim': x_val.claim, 'evidence': x_val.evidence}\n",
        "\n",
        "y_train = y_train.values\n",
        "y_val = y_val.values\n",
        "\n",
        "for dataset, side in itertools.product([x_train, x_val], ['claim', 'evidence']):\n",
        "  dataset[side] = pad_sequences(dataset[side], maxlen=max_seq)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=3)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNz0pz-mxw8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "class LossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = {'batch':[], 'epoch':[]}\n",
        "        self.accuracy = {'batch':[], 'epoch':[]}\n",
        "        self.val_loss = {'batch':[], 'epoch':[]}\n",
        "        self.val_acc = {'batch':[], 'epoch':[]}\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses['batch'].append(logs.get('loss'))\n",
        "        self.accuracy['batch'].append(logs.get('acc'))\n",
        "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
        "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.losses['epoch'].append(logs.get('loss'))\n",
        "        self.accuracy['epoch'].append(logs.get('acc'))\n",
        "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
        "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
        "\n",
        "    def loss_plot(self, loss_type, savepath):\n",
        "        iters = range(len(self.losses[loss_type]))\n",
        "        plt.figure()\n",
        "        # acc\n",
        "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
        "        # loss\n",
        "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
        "        # val_acc\n",
        "        plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
        "        # val_loss\n",
        "        plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
        "        plt.grid(True)\n",
        "        plt.xlabel(loss_type)\n",
        "        plt.ylabel('acc-loss')\n",
        "        plt.legend(loc=\"upper right\")    \n",
        "        plt.savefig(savepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN6pxK-Ix5D3",
        "colab_type": "code",
        "outputId": "5dc809ba-89e1-4312-dccc-a2ffa0dddfec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "# train the model and save the checkpoint with best improvement, then make a plot for the training\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "history = LossHistory()\n",
        "model_checkpoint = ModelCheckpoint('/content/gdrive/My Drive/esim_output/esim.hdf5', monitor='loss', verbose=1, save_best_only=True)\n",
        "start = time()\n",
        "train = model.fit([x_train[\"claim\"], x_train[\"evidence\"]], y_train, epochs=n_epoch, batch_size = 64,validation_data=([x_val[\"claim\"], x_val[\"evidence\"]], y_val),callbacks=[model_checkpoint, history])\n",
        "print(time()-start)\n",
        "history.loss_plot(\"epoch\",'/content/gdrive/My Drive/esim_output/esim_plot.jpg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 135000 samples, validate on 15000 samples\n",
            "Epoch 1/4\n",
            "135000/135000 [==============================] - 4965s 37ms/step - loss: 0.8533 - acc: 0.5854 - val_loss: 0.7610 - val_acc: 0.6382\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.85330, saving model to /content/gdrive/My Drive/esim_output/esim.hdf5\n",
            "Epoch 2/4\n",
            "135000/135000 [==============================] - 4801s 36ms/step - loss: 0.7057 - acc: 0.6723 - val_loss: 0.6782 - val_acc: 0.6928\n",
            "\n",
            "Epoch 00002: loss improved from 0.85330 to 0.70571, saving model to /content/gdrive/My Drive/esim_output/esim.hdf5\n",
            "Epoch 3/4\n",
            "135000/135000 [==============================] - 4921s 36ms/step - loss: 0.5938 - acc: 0.7367 - val_loss: 0.6617 - val_acc: 0.7087\n",
            "\n",
            "Epoch 00003: loss improved from 0.70571 to 0.59376, saving model to /content/gdrive/My Drive/esim_output/esim.hdf5\n",
            "Epoch 4/4\n",
            "135000/135000 [==============================] - 4969s 37ms/step - loss: 0.5051 - acc: 0.7825 - val_loss: 0.6421 - val_acc: 0.7249\n",
            "\n",
            "Epoch 00004: loss improved from 0.59376 to 0.50506, saving model to /content/gdrive/My Drive/esim_output/esim.hdf5\n",
            "19672.034198522568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OriaFheTGKn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure all the test input has the max_seq length\n",
        "x_test = test_df[cols]\n",
        "x_test = x_test[:220000]\n",
        "x_test = {'claim': x_test.claim, 'evidence': x_test.evidence}\n",
        "for dataset, side in itertools.product([x_test], ['claim', 'evidence']):\n",
        "  dataset[side] = pad_sequences(dataset[side], maxlen=max_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvgaYzAafIy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict the reuslt \n",
        "predict = model.predict([x_test[\"claim\"], x_test[\"evidence\"]],batch_size=64,verbose =1)\n",
        "print(predict.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aoUizm9GWC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find the maximum value position in the array\n",
        "def find_max_pos(array):\n",
        "  m = max(array)\n",
        "  p = 0\n",
        "  for i in range(0,len(array)):\n",
        "    if (array[i] == m):\n",
        "      p = i\n",
        "  return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFNfT5OSGeZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a new column in the unlabelled test data\n",
        "test_label = []\n",
        "for array in predict:\n",
        "  pos = find_max_pos(array)\n",
        "  test_label.append(pos)\n",
        "\n",
        "print(len(test_label))\n",
        "\n",
        "result_df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/test-unlabelled.csv')\n",
        "result_df['label'] = test_label\n",
        "print(result_df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBKIkHAEG2nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# intialized the dictionary to store the output\n",
        "r = {}\n",
        "cur_key = \"\"\n",
        "for index, row in result_df.iterrows():\n",
        "  # just update evidence when the key does not change\n",
        "  if row['key'] == cur_key:\n",
        "    tmp = []\n",
        "    tmp.append(row['title'])\n",
        "    tmp.append(row['page'])\n",
        "    tmp.append(row['label'])\n",
        "    r[cur_key]['evidence'].append(tmp)\n",
        "  \n",
        "  # when key changed, we need add new member\n",
        "  else:\n",
        "      r[cur_key] = {}\n",
        "      r[cur_key]['claim'] = r[cur_key].setdefault('claim', row['claim'])\n",
        "      r[cur_key]['evidence'] =  r[cur_key].setdefault('evidence', [])\n",
        "      tmp = []\n",
        "      tmp.append(row['title'])\n",
        "      tmp.append(row['page'])\n",
        "      tmp.append(row['label'])\n",
        "      r[cur_key]['evidence'].append(tmp)\n",
        "\n",
        "# change the format of the dictionary\n",
        "keys = r.keys()\n",
        "for key in keys:\n",
        "  # remove not enough info\n",
        "  if len(r[key]['evidence']) > 0:\n",
        "    for item in r[key]['evidence'].copy():\n",
        "      # if tagged not enough then remove\n",
        "      if item[2] == 0:\n",
        "        r[key]['evidence'].remove(item)\n",
        "    \n",
        "    if len(r[key]['evidence']) == 0:\n",
        "      r[key]['label'] = r[key].setdefault('label', \"NOT ENOUGH INFO\")\n",
        "      \n",
        "    else:\n",
        "      temp = set()\n",
        "      for item in r[key]['evidence']:\n",
        "        temp.add(item[2])\n",
        "      if len(temp) == 2:\n",
        "        r[key]['label'] = r[key].setdefault('label', \"NOT ENOUGH INFO\")\n",
        "        r[key]['evidence'] = []\n",
        "      elif len(temp) == 1:\n",
        "        l = list(temp)[0]\n",
        "        if l == 1:\n",
        "          r[key]['label'] = \"REFUTES\"\n",
        "          for i in range(len(r[key]['evidence'])):\n",
        "            r[key]['evidence'][i] = r[key]['evidence'][i][:2]\n",
        "        elif l == 2:\n",
        "          r[key]['label'] =  \"SUPPORTS\"\n",
        "          for i in range(len(r[key]['evidence'])):\n",
        "            r[key]['evidence'][i] = r[key]['evidence'][i][:2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLEyZWYfG61Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "json_str = json.dumps(r, indent=4)\n",
        "with open('/content/gdrive/My Drive/esim_output/testoutput.json', 'w') as json_file:\n",
        "    json_file.write(json_str)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}